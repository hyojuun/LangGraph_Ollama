{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Llama_tutorial\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U langchain langgraph langsmith langchain_experimental langchain_core langchain_ollama Ipython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "def langsmith(project_name=None, set_enable=True):\n",
    "    load_dotenv()\n",
    "\n",
    "    if set_enable:\n",
    "        result = os.environ.get(\"LANGCHAIN_API_KEY\")\n",
    "        if result is None or result.strip() == \"\":\n",
    "            print(\n",
    "                \"LangChain API Key가 설정되지 않았습니다.\"\n",
    "            )\n",
    "            return\n",
    "        os.environ[\"LANGCHAIN_ENDPOINT\"] = (\n",
    "            \"https://api.smith.langchain.com\"  # LangSmith API 엔드포인트\n",
    "        )\n",
    "        os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"  # true: 활성화\n",
    "        os.environ[\"LANGCHAIN_PROJECT\"] = project_name  # 프로젝트명\n",
    "        print(f\"LangSmith 추적을 시작합니다.\\n[프로젝트명]\\n{project_name}\")\n",
    "    else:\n",
    "        os.environ[\"LANGCHAIN_TRACING_V2\"] = \"false\"  # false: 비활성화\n",
    "        print(\"LangSmith 추적을 하지 않습니다.\")\n",
    "\n",
    "\n",
    "def env_variable(key, value):\n",
    "    os.environ[key] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangSmith 추적을 시작합니다.\n",
      "[프로젝트명]\n",
      "Llama_Tutorial\n"
     ]
    }
   ],
   "source": [
    "langsmith(\"Llama_Tutorial\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "# 경고 메시지 무시\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Web Search Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "web_search_tool = TavilySearchResults()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Llama3.1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm here to help with a question. However, it seems like \"hi\" is more of a greeting than a specific query. Could you please rephrase or provide more context for me to assist you better?\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers.string import StrOutputParser\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "# PromptTemplate 수정: input_variables를 리스트로 변경\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"You are an assistant for question-answering tasks.\n",
    "\n",
    "    Use the following documents to answer the question.\n",
    "\n",
    "    If you don't know the answer, just say that you don't know.\n",
    "\n",
    "    Use three sentences maximum and keep the answer concise:\n",
    "    Question: {question}\n",
    "    Answer:\n",
    "    \"\"\",\n",
    "    input_variables=[\"question\"]  # 리스트로 변경\n",
    ")\n",
    "\n",
    "# LLM 인스턴스 생성\n",
    "llm = ChatOllama(\n",
    "    model=\"llama3.1\",\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "# StrOutputParser를 인스턴스화하고 체인 생성\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "# 입력을 딕셔너리로 전달\n",
    "result = chain.invoke({\"question\": \"hi\"})\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Retrieval Grader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "\n",
    "# JSON\n",
    "llm = ChatOllama(\n",
    "    model='llama3.1',\n",
    "    format='json',\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"You are an grader assesing relevance of the retrieved document to a user question. \\n\n",
    "    Here is the retrieved document: \\n\\n {document} \\n\\n\n",
    "    Here is the user question: \\n\\n {question} \\n\\n\n",
    "    If the document contains keywords related to the user question, grade is as relevant. \\n\n",
    "    It does not need to be a stringest test. The goal is to filter out erroneous retrievals. \\n\n",
    "    Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question. \\n\n",
    "    Provide the binary score as a JSON with a single key 'score' and no premable or explanation.\n",
    "    \"\"\", \n",
    "    input_variables={\"document\", \"question\"}\n",
    ")\n",
    "\n",
    "retrieval_grader = prompt | llm | JsonOutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. LangGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, List\n",
    "from IPython.display import Image, display\n",
    "from langgraph.graph import START, END, StateGraph\n",
    "\n",
    "class GraphState(TypedDict):\n",
    "    \"\"\"\n",
    "    Represents the state of our graph\n",
    "    \n",
    "    Attributes:\n",
    "        question: question\n",
    "        generation: LLM generation\n",
    "        search: whether to add search\n",
    "        documents: list of documents\n",
    "    \"\"\"\n",
    "    \n",
    "    question: str\n",
    "    generation: str\n",
    "    search: str\n",
    "    documents: List[str]\n",
    "    steps: List[str]\n",
    "    \n",
    "def retrieve(state: GraphState):\n",
    "    \"\"\"\n",
    "    Retrieves documents\n",
    "    \n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "        \n",
    "    Returns:\n",
    "        state (dict): New key added to state, documents, that contains the retrieved documents\n",
    "    \"\"\"\n",
    "    \n",
    "    question = state[\"question\"]\n",
    "    documents = retriever.invoke(question)\n",
    "    steps = state['steps']\n",
    "    steps.append(\"retrieve_documents\")\n",
    "    return {\"documents\": documents, \"question\": question, \"steps\": steps}\n",
    "    \n",
    "def generate(state: GraphState):\n",
    "    \"\"\"\n",
    "    Generate answer\n",
    "    \n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "    \n",
    "    Returns:\n",
    "        state (dict): New key added to state, generation, that contains the LLM generation\n",
    "    \"\"\"\n",
    "    \n",
    "    question = state[\"question\"]\n",
    "    documents = state['documents']\n",
    "    generation = rag_chain.invoke({\"documents\": documents, \"question\": question})\n",
    "    steps = state[\"steps\"]\n",
    "    steps.append(\"generate_answer\")\n",
    "    return {\n",
    "        \"documents\": documents, \n",
    "        \"question\": question, \n",
    "        \"generation\": generation, \n",
    "        \"steps\": steps\n",
    "    }\n",
    "\n",
    "def grade_documents(state: GraphState):\n",
    "    \"\"\"\n",
    "    Determines whether the retrieved documents are relevant to the question.\n",
    "    \n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "        \n",
    "    Returns:\n",
    "        state (dict): Updates documents key with onlt filtered relevant documents\n",
    "    \"\"\"\n",
    "    \n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    steps = state[\"steps\"]\n",
    "    steps.append(\"grade_documents_retrieval\")\n",
    "    filtered_docs = []\n",
    "    search = \"No\"\n",
    "    for doc in documents:\n",
    "        score = retrieval_grader.invoke(\n",
    "            {\"question\": question, \"document\": doc.page_content}\n",
    "        )\n",
    "        grade = score[\"score\"]\n",
    "        if grade == \"Yes\":\n",
    "            filtered_docs.append(doc)\n",
    "        else:\n",
    "            search = \"Yes\"\n",
    "            continue\n",
    "    return {\n",
    "        \"documents\": filtered_docs, \n",
    "        \"question\": question,\n",
    "        \"search\": search, \n",
    "        \"steps\": steps\n",
    "    }\n",
    "    \n",
    "def web_search(state: GraphState):\n",
    "    \"\"\"\n",
    "    Web search based on the re-phrased question\n",
    "    \n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "    \n",
    "    Returns:\n",
    "        state (dict): Updates documents key with appended web results\n",
    "    \"\"\"\n",
    "    \n",
    "    question = state[\"question\"]\n",
    "    documents = state.get(\"documents\", [])\n",
    "    steps = state[\"steps\"]\n",
    "    steps.append(\"web_search\")\n",
    "    web_results = web_search_tool.invoke({\"query\": question})\n",
    "    documents.extend(\n",
    "        [\n",
    "            Document(page_content=doc['content'], metadata={\"url\": doc[\"url\"]})\n",
    "            for doc in web_results\n",
    "        ]\n",
    "    )\n",
    "    return {\n",
    "        \"documents\": documents, \n",
    "        \"question\": question, \n",
    "        \"steps\": steps\n",
    "    }\n",
    "    \n",
    "def decide_to_generate(state: GraphState):\n",
    "    \"\"\" \n",
    "    Determines whether to generate an answer or re-generate a question.\n",
    "    \n",
    "    Args:\n",
    "        staet (dict): The current graph state\n",
    "    \n",
    "    Returns:\n",
    "        str: Binary decision for next node to call\n",
    "    \"\"\"\n",
    "    \n",
    "    search = state[\"search\"]\n",
    "    if search == \"Yes\":\n",
    "        return \"search\"\n",
    "    else:\n",
    "        return \"generate\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Graph Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph\n",
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "# Define the nodes\n",
    "workflow.add_node(\"retrieve\", retrieve)\n",
    "workflow.add_node(\"generate\", generate)\n",
    "workflow.add_node(\"grade_documents\", grade_documents)\n",
    "workflow.add_node(\"web_search\", web_search)\n",
    "\n",
    "# Define the edges\n",
    "workflow.set_entry_point(\"retrieve\")\n",
    "workflow.add_edge(\"retrieve\", \"grade_documents\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"grade_documents\",\n",
    "    decide_to_generate, \n",
    "    {\n",
    "        \"search\": \"web_search\", \n",
    "        \"generate\": \"generate\"\n",
    "    }\n",
    ")\n",
    "workflow.add_edge(\"web_search\", \"generate\")\n",
    "workflow.add_edge(\"generate\", END)\n",
    "\n",
    "custom_graph = workflow.compile()\n",
    "\n",
    "display(Image(custom_graph.get_graph(xray=True).draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "def predict_custom_agent_answer(example: dict):\n",
    "    \n",
    "    config = {\"configureable\": {\"thread_id\": str(uuid.uuid4())}}\n",
    "    \n",
    "    state_dict = custom_graph.invoke(\n",
    "        {\"question\": example[\"input\"], \"steps\": []}, config\n",
    "    )\n",
    "    \n",
    "    return {\"response\": state_dict[\"generation\"], \"steps\": state_dict[\"steps\"]}\n",
    "\n",
    "example = {\"input\": \"What is the capital of France?\"}\n",
    "response = predict_custom_agent_answer(example)\n",
    "print(response)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "\n",
    "# Grade prompt\n",
    "grade_prompt_answer_accuracy = hub.pull(\"langchain-ai/rag-answer-vs-reference\")\n",
    "\n",
    "def anser_evaluator(run, example) -> dict:\n",
    "    \"\"\"\n",
    "    A simple evaluator for RAG answer accuracy\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get the question, the ground truth reference answer, RAG chain answer prediction\n",
    "    input_question = exmaple.inputs['input']\n",
    "    reference = example.outputs['output']\n",
    "    prediction = run.outputs[\"response\"]\n",
    "    \n",
    "    # Define an LLM grader\n",
    "    llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
    "    answer_grader = grade_prompt_answer_accuracy | llm\n",
    "    \n",
    "    # Run evaluator\n",
    "    score = answer_grader.invoke(\n",
    "        {\n",
    "            \"question\": input_question, \n",
    "            \"correct_answer\": reference, \n",
    "            \"student_answer\": prediction,\n",
    "        }\n",
    "    )\n",
    "    score = score[\"Score\"]\n",
    "    return {\"key\": \"answer_vs_reference_score\", \"score\": score}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 8-1. Custom Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import Example, Run\n",
    "\n",
    "# Reasoning traces that we expect the agents to take\n",
    "expected_trajectory_1 = [\n",
    "    \"retrieve_documents\", \n",
    "    \"grade_documents_retrieval\", \n",
    "    \"web_search\", \n",
    "    \"generate_answer\"\n",
    "]\n",
    "\n",
    "expected_trajectory_2 = [\n",
    "    \"retrieve_documents\", \n",
    "    \"grade_documents_retrieval\", \n",
    "    \"generate_answer\"\n",
    "]\n",
    "\n",
    "def check_trajectory_custom(root_run: Run, example: Example) -> dict:\n",
    "    \"\"\"\n",
    "    Check if all expected tools are called in exact order and without any additional tool calls.\n",
    "    \"\"\"\n",
    "    \n",
    "    tool_calls = root_run.outputs[\"steps\"]\n",
    "    print(f\"Tool calls custom agent: {tool_calls}\")\n",
    "    if tool_calls == expected_trajectory_1 or tool_calls == expected_trajectory_2:\n",
    "        score = 1\n",
    "    else:\n",
    "        score = 0\n",
    "    \n",
    "    return {\"score\": int(score), \"key\": \"tool_calls_in_exact_order\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Create Langsmith Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import Client\n",
    "\n",
    "client = Client()\n",
    "\n",
    "# Create a dataset\n",
    "examples = [\n",
    "    (\n",
    "        \"How does the ReAct agent use self-reflection?\", \n",
    "        \"ReAct integrates reasoning and acting, performing actions - such tools like Wikipedia search API\"\n",
    "    ), \n",
    "    (\n",
    "        # more examples...\n",
    "    )\n",
    "]\n",
    "\n",
    "# Save it\n",
    "dataset_name = \"Corrective RAG Agent Testing\"\n",
    "if not client.has_dataset(dataset_name=dataset_name):\n",
    "    dataset = client.create_dataset(dataset_name=dataset_name)\n",
    "    inputs, outputs = zip(\n",
    "        *[({\"input\": text}, {\"output\":label}) for text, label in examples]\n",
    "    )\n",
    "    client.create_examples[inputs=inputs, outputs=outputs, dataset_id=dataset.id]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Run Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith.evaluation import evaluate\n",
    "\n",
    "dataset_name = \"Corrective RAG Agent Testing\"\n",
    "model_tested = \"llama3.1\"\n",
    "metadata = \"CRAG, llama3.1\"\n",
    "experiment_prefix = f\"custom-agent-{model_tested}\"\n",
    "experiment_results = evaluate(\n",
    "    predict_custom_agent_answer, \n",
    "    data=dataset_name, \n",
    "    evaluator=[answer_evaluator, check_trajectory_custom],\n",
    "    experiment_prefix=experiment_prefix + \"-answer-and-tool-use\",\n",
    "    num_repetitions=3, \n",
    "    metadata={\"version\": metadata}\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Ollama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

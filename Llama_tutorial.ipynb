{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Llama_tutorial\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in /Users/hyojuuun/anaconda3/envs/Ollama/lib/python3.13/site-packages (0.3.7)\n",
      "Requirement already satisfied: langgraph in /Users/hyojuuun/anaconda3/envs/Ollama/lib/python3.13/site-packages (0.2.53)\n",
      "Requirement already satisfied: langsmith in /Users/hyojuuun/anaconda3/envs/Ollama/lib/python3.13/site-packages (0.1.144)\n",
      "Requirement already satisfied: langchain_experimental in /Users/hyojuuun/anaconda3/envs/Ollama/lib/python3.13/site-packages (0.3.3)\n",
      "Requirement already satisfied: langchain_core in /Users/hyojuuun/anaconda3/envs/Ollama/lib/python3.13/site-packages (0.3.19)\n",
      "Collecting langchain_ollama\n",
      "  Downloading langchain_ollama-0.2.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /Users/hyojuuun/anaconda3/envs/Ollama/lib/python3.13/site-packages (from langchain) (6.0.2)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /Users/hyojuuun/anaconda3/envs/Ollama/lib/python3.13/site-packages (from langchain) (2.0.35)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /Users/hyojuuun/anaconda3/envs/Ollama/lib/python3.13/site-packages (from langchain) (3.11.6)\n",
      "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.0 in /Users/hyojuuun/anaconda3/envs/Ollama/lib/python3.13/site-packages (from langchain) (0.3.2)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.26.0 in /Users/hyojuuun/anaconda3/envs/Ollama/lib/python3.13/site-packages (from langchain) (1.26.4)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /Users/hyojuuun/anaconda3/envs/Ollama/lib/python3.13/site-packages (from langchain) (2.10.0)\n",
      "Requirement already satisfied: requests<3,>=2 in /Users/hyojuuun/anaconda3/envs/Ollama/lib/python3.13/site-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /Users/hyojuuun/anaconda3/envs/Ollama/lib/python3.13/site-packages (from langchain) (9.0.0)\n",
      "Requirement already satisfied: langgraph-checkpoint<3.0.0,>=2.0.4 in /Users/hyojuuun/anaconda3/envs/Ollama/lib/python3.13/site-packages (from langgraph) (2.0.5)\n",
      "Requirement already satisfied: langgraph-sdk<0.2.0,>=0.1.32 in /Users/hyojuuun/anaconda3/envs/Ollama/lib/python3.13/site-packages (from langgraph) (0.1.36)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/hyojuuun/anaconda3/envs/Ollama/lib/python3.13/site-packages (from langsmith) (0.27.2)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /Users/hyojuuun/anaconda3/envs/Ollama/lib/python3.13/site-packages (from langsmith) (3.10.11)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /Users/hyojuuun/anaconda3/envs/Ollama/lib/python3.13/site-packages (from langsmith) (1.0.0)\n",
      "Requirement already satisfied: langchain-community<0.4.0,>=0.3.0 in /Users/hyojuuun/anaconda3/envs/Ollama/lib/python3.13/site-packages (from langchain_experimental) (0.3.7)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /Users/hyojuuun/anaconda3/envs/Ollama/lib/python3.13/site-packages (from langchain_core) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /Users/hyojuuun/anaconda3/envs/Ollama/lib/python3.13/site-packages (from langchain_core) (24.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /Users/hyojuuun/anaconda3/envs/Ollama/lib/python3.13/site-packages (from langchain_core) (4.12.2)\n",
      "Requirement already satisfied: ollama<1,>=0.3.0 in /Users/hyojuuun/anaconda3/envs/Ollama/lib/python3.13/site-packages (from langchain_ollama) (0.3.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /Users/hyojuuun/anaconda3/envs/Ollama/lib/python3.13/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/hyojuuun/anaconda3/envs/Ollama/lib/python3.13/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/hyojuuun/anaconda3/envs/Ollama/lib/python3.13/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/hyojuuun/anaconda3/envs/Ollama/lib/python3.13/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/hyojuuun/anaconda3/envs/Ollama/lib/python3.13/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /Users/hyojuuun/anaconda3/envs/Ollama/lib/python3.13/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (0.2.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /Users/hyojuuun/anaconda3/envs/Ollama/lib/python3.13/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.17.2)\n",
      "Requirement already satisfied: anyio in /Users/hyojuuun/anaconda3/envs/Ollama/lib/python3.13/site-packages (from httpx<1,>=0.23.0->langsmith) (4.6.2.post1)\n",
      "Requirement already satisfied: certifi in /Users/hyojuuun/anaconda3/envs/Ollama/lib/python3.13/site-packages (from httpx<1,>=0.23.0->langsmith) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/hyojuuun/anaconda3/envs/Ollama/lib/python3.13/site-packages (from httpx<1,>=0.23.0->langsmith) (1.0.7)\n",
      "Requirement already satisfied: idna in /Users/hyojuuun/anaconda3/envs/Ollama/lib/python3.13/site-packages (from httpx<1,>=0.23.0->langsmith) (3.10)\n",
      "Requirement already satisfied: sniffio in /Users/hyojuuun/anaconda3/envs/Ollama/lib/python3.13/site-packages (from httpx<1,>=0.23.0->langsmith) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/hyojuuun/anaconda3/envs/Ollama/lib/python3.13/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /Users/hyojuuun/anaconda3/envs/Ollama/lib/python3.13/site-packages (from jsonpatch<2.0,>=1.33->langchain_core) (3.0.0)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /Users/hyojuuun/anaconda3/envs/Ollama/lib/python3.13/site-packages (from langchain-community<0.4.0,>=0.3.0->langchain_experimental) (0.6.7)\n",
      "Requirement already satisfied: httpx-sse<0.5.0,>=0.4.0 in /Users/hyojuuun/anaconda3/envs/Ollama/lib/python3.13/site-packages (from langchain-community<0.4.0,>=0.3.0->langchain_experimental) (0.4.0)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /Users/hyojuuun/anaconda3/envs/Ollama/lib/python3.13/site-packages (from langchain-community<0.4.0,>=0.3.0->langchain_experimental) (2.6.1)\n",
      "Requirement already satisfied: msgpack<2.0.0,>=1.1.0 in /Users/hyojuuun/anaconda3/envs/Ollama/lib/python3.13/site-packages (from langgraph-checkpoint<3.0.0,>=2.0.4->langgraph) (1.1.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/hyojuuun/anaconda3/envs/Ollama/lib/python3.13/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.0 in /Users/hyojuuun/anaconda3/envs/Ollama/lib/python3.13/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.27.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/hyojuuun/anaconda3/envs/Ollama/lib/python3.13/site-packages (from requests<3,>=2->langchain) (3.4.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/hyojuuun/anaconda3/envs/Ollama/lib/python3.13/site-packages (from requests<3,>=2->langchain) (2.2.3)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /Users/hyojuuun/anaconda3/envs/Ollama/lib/python3.13/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (3.23.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /Users/hyojuuun/anaconda3/envs/Ollama/lib/python3.13/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (0.9.0)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in /Users/hyojuuun/anaconda3/envs/Ollama/lib/python3.13/site-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (1.0.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /Users/hyojuuun/anaconda3/envs/Ollama/lib/python3.13/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (1.0.0)\n",
      "Downloading langchain_ollama-0.2.0-py3-none-any.whl (14 kB)\n",
      "Installing collected packages: langchain_ollama\n",
      "Successfully installed langchain_ollama-0.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install -U langchain langgraph langsmith langchain_experimental langchain_core langchain_ollama langchain_prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "def langsmith(project_name=None, set_enable=True):\n",
    "    load_dotenv()\n",
    "\n",
    "    if set_enable:\n",
    "        result = os.environ.get(\"LANGCHAIN_API_KEY\")\n",
    "        if result is None or result.strip() == \"\":\n",
    "            print(\n",
    "                \"LangChain API Key가 설정되지 않았습니다.\"\n",
    "            )\n",
    "            return\n",
    "        os.environ[\"LANGCHAIN_ENDPOINT\"] = (\n",
    "            \"https://api.smith.langchain.com\"  # LangSmith API 엔드포인트\n",
    "        )\n",
    "        os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"  # true: 활성화\n",
    "        os.environ[\"LANGCHAIN_PROJECT\"] = project_name  # 프로젝트명\n",
    "        print(f\"LangSmith 추적을 시작합니다.\\n[프로젝트명]\\n{project_name}\")\n",
    "    else:\n",
    "        os.environ[\"LANGCHAIN_TRACING_V2\"] = \"false\"  # false: 비활성화\n",
    "        print(\"LangSmith 추적을 하지 않습니다.\")\n",
    "\n",
    "\n",
    "def env_variable(key, value):\n",
    "    os.environ[key] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangSmith 추적을 시작합니다.\n",
      "[프로젝트명]\n",
      "Llama_Tutorial\n"
     ]
    }
   ],
   "source": [
    "langsmith(\"Llama_Tutorial\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "# 경고 메시지 무시\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_experimental.tools import PythonREPLTool\n",
    "\n",
    "python_tool = PythonREPLTool()\n",
    "\n",
    "print(python_tool.invoke(\"print(100 + 200)\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "BaseModel.__init__() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)\n",
      "Cell \u001b[0;32mIn[23], line 56\u001b[0m\n",
      "\u001b[1;32m     48\u001b[0m llm \u001b[38;5;241m=\u001b[39m ChatOllama(\n",
      "\u001b[1;32m     49\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllama3.1\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n",
      "\u001b[1;32m     50\u001b[0m     \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjson\u001b[39m\u001b[38;5;124m'\u001b[39m,\n",
      "\u001b[1;32m     51\u001b[0m     temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m\n",
      "\u001b[1;32m     52\u001b[0m )\n",
      "\u001b[1;32m     54\u001b[0m chain \u001b[38;5;241m=\u001b[39m prompt \u001b[38;5;241m|\u001b[39m llm \u001b[38;5;241m|\u001b[39m StrOutputParser\n",
      "\u001b[0;32m---> 56\u001b[0m \u001b[43mchain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mwhat time is it now?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\n",
      "File \u001b[0;32m~/anaconda3/envs/Ollama/lib/python3.13/site-packages/langchain_core/runnables/base.py:3024\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n",
      "\u001b[1;32m   3022\u001b[0m             \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(step\u001b[38;5;241m.\u001b[39minvoke, \u001b[38;5;28minput\u001b[39m, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;32m   3023\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;32m-> 3024\u001b[0m             \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m   3025\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n",
      "\u001b[1;32m   3026\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\n",
      "File \u001b[0;32m~/anaconda3/envs/Ollama/lib/python3.13/site-packages/langchain_core/runnables/base.py:4713\u001b[0m, in \u001b[0;36mRunnableLambda.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n",
      "\u001b[1;32m   4699\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Invoke this Runnable synchronously.\u001b[39;00m\n",
      "\u001b[1;32m   4700\u001b[0m \n",
      "\u001b[1;32m   4701\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n",
      "\u001b[0;32m   (...)\u001b[0m\n",
      "\u001b[1;32m   4710\u001b[0m \u001b[38;5;124;03m    TypeError: If the Runnable is a coroutine function.\u001b[39;00m\n",
      "\u001b[1;32m   4711\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n",
      "\u001b[1;32m   4712\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunc\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "\u001b[0;32m-> 4713\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_with_config\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[1;32m   4714\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_invoke\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   4715\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   4716\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   4717\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   4718\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m   4719\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;32m   4720\u001b[0m     msg \u001b[38;5;241m=\u001b[39m (\n",
      "\u001b[1;32m   4721\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot invoke a coroutine function synchronously.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;32m   4722\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUse `ainvoke` instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;32m   4723\u001b[0m     )\n",
      "\n",
      "File \u001b[0;32m~/anaconda3/envs/Ollama/lib/python3.13/site-packages/langchain_core/runnables/base.py:1927\u001b[0m, in \u001b[0;36mRunnable._call_with_config\u001b[0;34m(self, func, input, config, run_type, serialized, **kwargs)\u001b[0m\n",
      "\u001b[1;32m   1923\u001b[0m     context \u001b[38;5;241m=\u001b[39m copy_context()\n",
      "\u001b[1;32m   1924\u001b[0m     context\u001b[38;5;241m.\u001b[39mrun(_set_config_context, child_config)\n",
      "\u001b[1;32m   1925\u001b[0m     output \u001b[38;5;241m=\u001b[39m cast(\n",
      "\u001b[1;32m   1926\u001b[0m         Output,\n",
      "\u001b[0;32m-> 1927\u001b[0m         \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[1;32m   1928\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcall_func_with_variable_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n",
      "\u001b[1;32m   1929\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n",
      "\u001b[1;32m   1930\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n",
      "\u001b[1;32m   1931\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1932\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1933\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1934\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m,\n",
      "\u001b[1;32m   1935\u001b[0m     )\n",
      "\u001b[1;32m   1936\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[1;32m   1937\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n",
      "\n",
      "File \u001b[0;32m~/anaconda3/envs/Ollama/lib/python3.13/site-packages/langchain_core/runnables/config.py:396\u001b[0m, in \u001b[0;36mcall_func_with_variable_args\u001b[0;34m(func, input, config, run_manager, **kwargs)\u001b[0m\n",
      "\u001b[1;32m    394\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m accepts_run_manager(func):\n",
      "\u001b[1;32m    395\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m run_manager\n",
      "\u001b[0;32m--> 396\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "File \u001b[0;32m~/anaconda3/envs/Ollama/lib/python3.13/site-packages/langchain_core/runnables/base.py:4567\u001b[0m, in \u001b[0;36mRunnableLambda._invoke\u001b[0;34m(self, input, run_manager, config, **kwargs)\u001b[0m\n",
      "\u001b[1;32m   4565\u001b[0m                 output \u001b[38;5;241m=\u001b[39m chunk\n",
      "\u001b[1;32m   4566\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;32m-> 4567\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mcall_func_with_variable_args\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[1;32m   4568\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n",
      "\u001b[1;32m   4569\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m   4570\u001b[0m \u001b[38;5;66;03m# If the output is a Runnable, invoke it\u001b[39;00m\n",
      "\u001b[1;32m   4571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, Runnable):\n",
      "\n",
      "File \u001b[0;32m~/anaconda3/envs/Ollama/lib/python3.13/site-packages/langchain_core/runnables/config.py:396\u001b[0m, in \u001b[0;36mcall_func_with_variable_args\u001b[0;34m(func, input, config, run_manager, **kwargs)\u001b[0m\n",
      "\u001b[1;32m    394\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m accepts_run_manager(func):\n",
      "\u001b[1;32m    395\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m run_manager\n",
      "\u001b[0;32m--> 396\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "File \u001b[0;32m~/anaconda3/envs/Ollama/lib/python3.13/site-packages/langchain_core/load/serializable.py:125\u001b[0m, in \u001b[0;36mSerializable.__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;32m    124\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\"\"\"\u001b[39;00m\n",
      "\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "\u001b[0;31mTypeError\u001b[0m: BaseModel.__init__() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "# from langchain_core.runnables import RunnaleLambda\n",
    "\n",
    "# import ollama\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "def print_and_execute(code, debug=True):\n",
    "    if debug:\n",
    "        print(\"CODE:\")\n",
    "        print(code)\n",
    "    return python_tool.invoke(code)\n",
    "\n",
    "# prompt = ChatPromptTemplate.from_messages(\n",
    "#     [\n",
    "#         (\n",
    "#             \"system\",\n",
    "#             \"You are Raymond Hetting, an expert python programmer, well versed in meta-programming and elegant, concise and short but well documented code. You follow the PEP8 style guide. \"\n",
    "#             \"Return only the code, no intro, no explanation, no chatty, no markdown, no code block, no nothing. Just the code.\",\n",
    "#         ),\n",
    "#         (\"human\", \"{input}\"),\n",
    "#     ]\n",
    "# )\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"You are an assistant for question-answering tasks.\n",
    "    \n",
    "    Use the following documents to answer the question.\n",
    "    \n",
    "    If you don't know the answer. just say that you don't know.\n",
    "    \n",
    "    Use theree senteces maximum and keep the answer concise:\n",
    "    Question: {question}\n",
    "    Answer:\n",
    "    \"\"\",\n",
    "    input_variables={\"question\"}\n",
    ")\n",
    "\n",
    "# response = ollama.chat(\n",
    "#     model='llama3.2-vision',\n",
    "#     messages=[{\n",
    "#         'role': 'user',\n",
    "#         'content': 'What is in this image?'\n",
    "#     }]\n",
    "# )\n",
    "\n",
    "llm = ChatOllama(\n",
    "    model = \"llama3.1\",\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "chain = prompt | llm | StrOutputParser\n",
    "\n",
    "chain.invoke(\"what time is it now?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "\n",
    "# JSON\n",
    "llm = ChatOllama(\n",
    "    model='llama3.1',\n",
    "    format='json',\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"\n",
    "    \"\"\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Ollama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
